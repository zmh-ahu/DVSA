## **DVSA: Diagonal and Vertical Self-Attention**



This repository provides an implementation of **Diagonal and Vertical Self-Attention (DVSA)**, a variant of the attention mechanism designed for efficient and focused context modeling.

The core implementation is contained in a single file: **`DVSA.py`**.
 Running this script will directly compute and display the **Q, K, V matrices** along with their attention outputs.



## ðŸš€ Usage

### 1. Clone the repository

```
git clone https://github.com/your-username/DVSA.git
cd DVSA
```



### 2. Run the code

```
python DVSA.py
```

This will print the computed **Q, K, V** results of the DVSA mechanism to the console.



## ðŸ“– Reference

If you use or extend this implementation in your research, please cite the following paper:

**APA Style**
 Zhang, M., Song, J., Xie, F., Shi, K., Guo, Z., & Weng, F. (2025). DVSA: A focused and efficient sparse attention via explicit selection for speech recognition. *Speech Communication*, 103300. https://doi.org/10.1016/j.specom.2025.103300
